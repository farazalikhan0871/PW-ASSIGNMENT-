{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c7880a-a5ed-4b20-9def-05af4ecf8cc8",
   "metadata": {},
   "source": [
    "Q1. What is an activation function in the context of artificial neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f17891-214f-4431-9dbe-22a7f1e7ce39",
   "metadata": {},
   "source": [
    "In the context of artificial neural networks, an activation function is a mathematical function that introduces non-linearity to the output of a neuron or a set of neurons. It is applied to the weighted sum of the inputs to a neuron and determines the output or activation level of the neuron.\n",
    "\n",
    "The activation function is a critical component of a neural network because it allows the network to model complex relationships between inputs and outputs. Without an activation function, a neural network would simply be a linear combination of its inputs, and its power and flexibility would be severely limited.\n",
    "\n",
    "By applying an activation function, the neural network can learn and approximate non-linear functions, enabling it to solve more complex tasks such as pattern recognition, classification, and regression. The non-linear activation function introduces non-linearities into the network, allowing it to capture intricate patterns and make decisions based on the input data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e835a9-4f6f-4ff5-9411-9f7683c5d413",
   "metadata": {},
   "source": [
    "Q2. What are some common types of activation functions used in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18e3b92-bd20-49a8-9050-24e7d08f87df",
   "metadata": {},
   "source": [
    "There are several common types of activation functions used in neural networks. Here are a few examples:\n",
    "Sigmoid Activation Function:\n",
    "The sigmoid function, also known as the logistic function, maps the input to a value between 0 and 1. It is useful in cases where the output needs to be interpreted as a probability. The sigmoid function is given by: f(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "Hyperbolic Tangent (tanh) Activation Function:\n",
    "The hyperbolic tangent function, or tanh, maps the input to a value between -1 and 1. It is similar to the sigmoid function but symmetric around the origin. The tanh function is given by: f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "Rectified Linear Unit (ReLU) Activation Function:\n",
    "The ReLU function is a piecewise linear function that returns the input value if it is positive and 0 otherwise. It is widely used due to its simplicity and computational efficiency. The ReLU function is given by: f(x) = max(0, x)\n",
    "\n",
    "Leaky ReLU Activation Function:\n",
    "The leaky ReLU function is a variant of the ReLU function that allows small negative values instead of setting them to 0. It helps mitigate the \"dying ReLU\" problem where neurons can become unresponsive during training. The leaky ReLU function is given by: f(x) = max(0.01x, x)\n",
    "\n",
    "Parametric ReLU (PReLU) Activation Function:\n",
    "The parametric ReLU function is an extension of the leaky ReLU function, where the slope for negative values is learned during training. It allows the network to adaptively determine the negative slope instead of using a fixed value. The PReLU function is given by: f(x) = max(a * x, x) (where a is a learnable parameter)\n",
    "\n",
    "Softmax Activation Function:\n",
    "The softmax function is commonly used in the output layer of a neural network for multi-class classification problems. It takes a vector of real numbers as input and normalizes it into a probability distribution over the classes. The softmax function is given by: f(x_i) = exp(x_i) / sum(exp(x_j)) (for each element x_i in the input vector)\n",
    "\n",
    "These are just a few examples of activation functions commonly used in neural networks. The choice of activation function depends on the nature of the problem, the desired properties of the network, and empirical performance on the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30273c50-e1ab-4601-b686-e56f2e60cd07",
   "metadata": {},
   "source": [
    "Q3. How do activation functions affect the training process and performance of a neural network? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0add761-7c8c-4684-bd8d-bbc09ca47f93",
   "metadata": {},
   "source": [
    "Activation functions play a crucial role in the training process and performance of a neural network. Here's how activation functions can affect these aspects:\n",
    "Non-linearity and Model Complexity:\n",
    "Activation functions introduce non-linearity into the network, allowing it to model complex relationships between inputs and outputs. Without non-linear activation functions, a neural network would be limited to representing only linear functions, severely reducing its expressive power. Non-linear activation functions enable the network to capture intricate patterns and make decisions based on the input data.\n",
    "\n",
    "Gradient Flow and Backpropagation:\n",
    "During the training process, neural networks use backpropagation to update the weights based on the calculated gradients. The choice of activation function affects the flow of gradients through the network. Some activation functions, such as the sigmoid function, suffer from the vanishing gradient problem, where gradients diminish as they propagate backward through layers, making it harder to train deep networks. Activation functions like ReLU alleviate this problem by providing a larger gradient magnitude for positive inputs.\n",
    "\n",
    "Training Speed and Convergence:\n",
    "The choice of activation function can impact the speed of convergence during training. Activation functions like ReLU tend to converge faster than sigmoid or tanh functions. This is because ReLU activation eliminates the vanishing gradient problem and accelerates learning by allowing more efficient gradient propagation.\n",
    "\n",
    "Output Range and Scaling:\n",
    "The output range of an activation function can affect the scaling of network activations. Activation functions like sigmoid and tanh squash the input into a limited range, which can cause saturation and slow down training if the inputs are not properly scaled. ReLU and its variants do not have this issue as they do not saturate.\n",
    "\n",
    "Sparsity and Network Efficiency:\n",
    "Activation functions like ReLU can induce sparsity in neural networks by setting negative values to zero. This sparsity can make the network more efficient by reducing the number of active neurons and the computational load. Sparse activations can also enhance interpretability by focusing on relevant features in the input.\n",
    "\n",
    "Task Suitability:\n",
    "Different activation functions may perform better or worse depending on the specific task at hand. For example, sigmoid and tanh functions are commonly used in the output layer for binary classification and multi-class classification problems, respectively. Softmax activation is often used for multi-class classification. The choice of activation function should align with the requirements of the task and the properties of the data.\n",
    "\n",
    "It's important to note that there is no universally optimal activation function for all scenarios. The selection of the activation function depends on the characteristics of the problem, the nature of the data, and empirical performance through experimentation and tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79679fc-7989-4ffd-8c54-b5ad38de6452",
   "metadata": {},
   "source": [
    "Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a87f65b-d422-4bb7-adaa-8fb59acfa8e1",
   "metadata": {},
   "source": [
    "The sigmoid activation function, also known as the logistic function, is a widely used activation function in neural networks. It maps the input to a value between 0 and 1, which can be interpreted as a probability. Here's how the sigmoid activation function works:\n",
    "The sigmoid function is given by the formula:\n",
    "f(x) = 1 / (1 + exp(-x))\n",
    "In this formula, 'x' represents the input to the function. The sigmoid function applies a mathematical operation known as the exponential function to the negative of the input, and then adds 1 to the result. Finally, it takes the reciprocal of the sum, giving a value between 0 and 1 as the output.\n",
    "Advantages of the sigmoid activation function:\n",
    "Bounded Output: The sigmoid function produces an output between 0 and 1, which can be interpreted as a probability or a binary decision. It is particularly useful in binary classification tasks, where the output represents the probability of belonging to a particular class.\n",
    "\n",
    "Smooth and Continuous: The sigmoid function is a smooth, differentiable function. This property makes it suitable for gradient-based optimization algorithms, such as backpropagation, which rely on derivatives for updating the weights during training.\n",
    "\n",
    "Disadvantages of the sigmoid activation function:\n",
    "Vanishing Gradient: One significant disadvantage of the sigmoid function is the vanishing gradient problem. When the input to the sigmoid function is large or small, the gradient of the function approaches zero, resulting in slow learning and convergence during training, especially in deep neural networks.\n",
    "\n",
    "Output Saturation: The sigmoid function saturates at the extreme values of the input. This means that for very large positive or negative inputs, the output of the sigmoid function becomes close to 1 or 0, respectively. In such cases, the gradient becomes close to zero, leading to limited learning and slower convergence.\n",
    "\n",
    "Biased Outputs: The sigmoid function maps negative inputs to values close to 0 and positive inputs to values close to 1. This bias towards positive inputs can cause issues when dealing with imbalanced data or when trying to model outputs that are not naturally bounded between 0 and 1.\n",
    "\n",
    "Computational Cost: The sigmoid function involves the computation of the exponential function, which can be computationally expensive compared to other activation functions like ReLU.\n",
    "\n",
    "Due to its drawbacks, the sigmoid activation function is not as commonly used in hidden layers of deep neural networks today. Other activation functions like ReLU and its variants have gained popularity due to their ability to mitigate the vanishing gradient problem and provide faster convergence. However, the sigmoid activation function is still used in the output layer for binary classification problems or when probabilities are required as outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d844b49-5a0b-4e5b-a5c3-b7af01c1f2bd",
   "metadata": {},
   "source": [
    "Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15954473-efab-4797-9163-c578289545f8",
   "metadata": {},
   "source": [
    "The rectified linear unit (ReLU) activation function is a popular activation function used in neural networks. It introduces non-linearity by returning the input value if it is positive, and 0 otherwise. In other words, it \"rectifies\" negative inputs to zero and keeps positive inputs unchanged. Here's how the ReLU activation function works:\n",
    "The ReLU function is given by the formula:\n",
    "f(x) = max(0, x)\n",
    "In this formula, 'x' represents the input to the function. If the input 'x' is positive, the ReLU function outputs 'x' itself. If 'x' is negative, the ReLU function outputs 0.\n",
    "Differences between ReLU and sigmoid activation functions:\n",
    "Range of Output:\n",
    "The sigmoid function maps the input to a value between 0 and 1, while the ReLU function outputs 0 for negative inputs and the input value itself for positive inputs. Therefore, the output range of the ReLU function is not bounded.\n",
    "\n",
    "Non-linearity:\n",
    "Both the sigmoid and ReLU functions introduce non-linearity, but in different ways. The sigmoid function provides a smooth, S-shaped curve, while the ReLU function is a piecewise linear function. The linearity of the ReLU function simplifies the computations and allows for more efficient training and convergence.\n",
    "\n",
    "Gradient Flow:\n",
    "The sigmoid function suffers from the vanishing gradient problem, especially for large or small input values, where the gradients approach zero. On the other hand, the ReLU function does not suffer from the vanishing gradient problem for positive inputs. It provides a constant gradient of 1 for positive inputs, which accelerates learning and convergence.\n",
    "\n",
    "Sparsity and Zero Neurons:\n",
    "The ReLU function induces sparsity in neural networks. When a negative input is encountered, the ReLU function outputs zero, effectively deactivating the neuron. This sparsity can make the network more efficient by reducing the number of active neurons and the computational load. In contrast, the sigmoid function does not produce zero outputs, and all neurons contribute to the computation.\n",
    "\n",
    "Computational Efficiency:\n",
    "The ReLU function is computationally efficient compared to the sigmoid function. The ReLU function involves simple element-wise comparisons and does not require costly exponential calculations.\n",
    "\n",
    "Overall, the ReLU activation function has gained popularity in deep learning due to its simplicity, non-saturation of positive inputs, efficient computation, and ability to mitigate the vanishing gradient problem. However, it should be noted that the ReLU function can suffer from the \"dying ReLU\" problem, where neurons can become permanently inactive during training. This issue led to the development of variants like Leaky ReLU and Parametric ReLU (PReLU) to address the problem of dead neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4c53af-2699-44f1-91f7-6d7115ae1fd3",
   "metadata": {},
   "source": [
    "Q6. What are the benefits of using the ReLU activation function over the sigmoid function? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6a767-225e-4149-a067-b2b633111723",
   "metadata": {},
   "source": [
    "Using the ReLU (Rectified Linear Unit) activation function over the sigmoid function offers several benefits in the context of neural networks. Here are some key advantages of ReLU:\n",
    "1. Mitigation of Vanishing Gradient Problem:\n",
    "The ReLU activation function helps alleviate the vanishing gradient problem, which is common in deep neural networks. The vanishing gradient problem occurs when gradients become extremely small during backpropagation, hindering effective learning. ReLU's constant gradient of 1 for positive inputs enables more efficient gradient flow, allowing deeper networks to be trained effectively.\n",
    "\n",
    "2. Computational Efficiency:\n",
    "ReLU is computationally efficient compared to the sigmoid activation function. ReLU involves simple element-wise comparisons and does not require costly exponential calculations like sigmoid. The simplicity of ReLU makes it faster to compute, making training and inference more efficient, especially for large-scale neural networks.\n",
    "\n",
    "3. Sparse Activation:\n",
    "ReLU induces sparsity in neural networks. When a negative input is encountered, ReLU sets the output to zero, effectively deactivating the neuron. This sparsity leads to more efficient representation and computation, reducing the overall number of active neurons and the computational load. Sparse activation can also improve the interpretability of the network by focusing on relevant features.\n",
    "\n",
    "4. Avoidance of Saturation:\n",
    "The sigmoid activation function saturates at extreme input values, causing the gradient to approach zero. In contrast, ReLU does not saturate for positive inputs, enabling continued learning even with large input values. By avoiding saturation, ReLU facilitates faster convergence and prevents the network from getting stuck in a low-gradient region.\n",
    "\n",
    "5. Better Representation of Complex Functions:\n",
    "ReLU's linearity within the positive input range allows neural networks to learn and represent more complex functions. By combining ReLU activations across multiple layers, the network can approximate highly non-linear relationships between inputs and outputs effectively. This property makes ReLU well-suited for deep learning architectures.\n",
    "\n",
    "6. Support for Sparse Input Representations:\n",
    "ReLU can handle sparse input representations more effectively than the sigmoid function. Since ReLU sets negative inputs to zero, it effectively ignores irrelevant or noise-like features in the input, leading to improved performance on tasks with sparse or noisy data.\n",
    "\n",
    "While ReLU offers these advantages, it is important to note that it may lead to dead neurons (i.e., neurons that always output zero) during training, known as the \"dying ReLU\" problem. Several variations of ReLU, such as Leaky ReLU and Parametric ReLU (PReLU), have been introduced to address this issue while preserving the benefits of ReLU.\n",
    "Overall, the ReLU activation function has become a popular choice in modern neural networks due to its ability to mitigate the vanishing gradient problem, computational efficiency, sparsity, avoidance of saturation, and improved representation of complex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a5c1f-ee70-4f3b-accc-3691ebebec82",
   "metadata": {},
   "source": [
    "Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe11ea7-5ad3-42c5-bdb6-ed24abc99c47",
   "metadata": {},
   "source": [
    "Leaky ReLU is a variation of the rectified linear unit (ReLU) activation function that addresses the issue of \"dying ReLU\" and helps alleviate the vanishing gradient problem. In the standard ReLU function, negative inputs are completely set to zero, which can cause neurons to become unresponsive and \"die\" during training. Leaky ReLU introduces a small slope for negative inputs instead of setting them to zero, allowing a small, non-zero output. Here's how the leaky ReLU activation function works:\n",
    "\n",
    "The leaky ReLU function is given by the formula:\n",
    "f(x) = max(a * x, x)\n",
    "\n",
    "In this formula, 'x' represents the input to the function, and 'a' is a small constant (usually a small positive value like 0.01 or 0.001) that determines the slope of the function for negative inputs.\n",
    "\n",
    "By introducing a non-zero slope for negative inputs, leaky ReLU avoids completely deactivating neurons and prevents the \"dying ReLU\" problem. Here are the key aspects and benefits of leaky ReLU:\n",
    "\n",
    "1. Non-Zero Gradient for Negative Inputs:\n",
    "Unlike ReLU, which assigns a gradient of 0 for negative inputs, leaky ReLU assigns a small, non-zero gradient determined by the slope 'a'. This non-zero gradient ensures that gradients can flow backward through the network during backpropagation, allowing for continued learning and preventing the vanishing gradient problem.\n",
    "\n",
    "2. Mitigation of \"Dying ReLU\" Problem:\n",
    "The introduction of a non-zero slope for negative inputs in leaky ReLU prevents neurons from becoming completely inactive. This helps mitigate the issue of \"dying ReLU,\" where neurons stop learning and become unresponsive during training.\n",
    "\n",
    "3. Better Gradient Flow:\n",
    "The non-zero gradient for negative inputs in leaky ReLU ensures that gradients propagate backward through the network even for negative values. This improved gradient flow enables more stable and effective training of deep neural networks, especially in architectures with many layers.\n",
    "\n",
    "4. Improved Learning with Negative Inputs:\n",
    "By allowing small negative values in the output, leaky ReLU retains some information from negative inputs. This can be particularly useful in situations where negative input values contain meaningful information for the task at hand.\n",
    "\n",
    "5. Continuity and Differentiability:\n",
    "Leaky ReLU remains a continuous and differentiable function, just like ReLU. This property ensures that leaky ReLU can be used with gradient-based optimization algorithms like backpropagation for efficient weight updates during training.\n",
    "\n",
    "Leaky ReLU strikes a balance between the linearity and non-linearity of ReLU and sigmoid functions, respectively. It provides the benefits of ReLU (such as avoiding saturation and computational efficiency) while addressing the limitations of the standard ReLU function. The value of the slope 'a' in leaky ReLU is typically set based on empirical performance or through hyperparameter tuning.\n",
    "\n",
    "By introducing a small slope for negative inputs, leaky ReLU helps maintain a non-zero gradient, prevents neuron death, and facilitates better gradient flow during training, thereby addressing the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead6a5e-0cb5-498a-93a7-40316bd3262b",
   "metadata": {},
   "source": [
    "Q8. What is the purpose of the softmax activation function? When is it commonly used? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e8ee67-c523-4578-beb0-ec17fd9bb171",
   "metadata": {},
   "source": [
    "The softmax activation function is commonly used in the output layer of neural networks, particularly in multi-class classification tasks. It is designed to transform the raw output values of a neural network into a probability distribution over multiple classes, where the sum of the probabilities adds up to 1. The purpose of the softmax function is to provide a way to interpret the network's output as class probabilities. Here's how the softmax activation function works:\n",
    "\n",
    "Given a vector of raw output values (logits) from the preceding layer, the softmax function calculates the probability of each class as follows:\n",
    "f(x_i) = exp(x_i) / sum(exp(x_j))\n",
    "In this formula, 'x_i' represents the raw output value (logit) for class 'i', and 'exp' denotes the exponential function. The softmax function exponentiates each logit value, making them positive, and then normalizes them by dividing by the sum of all exponential values. This normalization ensures that the resulting probabilities lie between 0 and 1 and sum up to 1, representing a valid probability distribution.\n",
    "\n",
    "The softmax activation function is commonly used in multi-class classification problems where an input can belong to one of several mutually exclusive classes. Some examples include image classification, sentiment analysis, language modeling, and speech recognition. Here are a few reasons why the softmax function is used in these scenarios:\n",
    "\n",
    "1. Probability Interpretation:\n",
    "The softmax function converts the network's raw output values into probabilities, allowing us to interpret the output as the model's confidence or probability distribution over the different classes. This makes it easier to make class predictions and evaluate the model's uncertainty.\n",
    "\n",
    "2.Mutual Exclusivity:\n",
    "The softmax function assumes that the classes are mutually exclusive, meaning an input can only belong to one class. By normalizing the output to a probability distribution, softmax enforces this mutual exclusivity constraint.\n",
    "\n",
    "3. Gradient Computation:\n",
    "The softmax function is differentiable, which allows for the calculation of gradients during backpropagation. Gradients computed using softmax can be used to update the weights of the network through optimization algorithms like gradient descent.\n",
    "\n",
    "4. Loss Calculation:\n",
    "Softmax is often paired with the cross-entropy loss function in multi-class classification tasks. The softmax probabilities serve as inputs to the cross-entropy loss, which quantifies the difference between the predicted probabilities and the true class labels. The combination of softmax and cross-entropy loss helps guide the network's training by minimizing the loss and improving the model's predictive accuracy.\n",
    "\n",
    "It's important to note that the softmax function is not limited to the output layer of a neural network. In certain cases, it can also be used in intermediate layers when constructing hierarchical or structured predictions. However, its most common usage is in the final layer of a network for multi-class classification tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c0655a-7caa-4a74-b3aa-45ec6b19c62d",
   "metadata": {},
   "source": [
    "Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc7f5ed-6f49-4d90-9be1-4fec990e361c",
   "metadata": {},
   "source": [
    "The hyperbolic tangent (tanh) activation function is a commonly used activation function in neural networks. It is a rescaled version of the sigmoid function and maps the input to a value between -1 and 1. Here's how the tanh activation function works:\n",
    "\n",
    "The tanh function is given by the formula:\n",
    "\n",
    "f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "In this formula, 'x' represents the input to the function. The tanh function applies the exponential function to both the positive and negative values of the input, computes their difference, and divides it by their sum, resulting in a value between -1 and 1.\n",
    "Comparison of tanh and sigmoid activation functions:\n",
    "\n",
    "1. Output Range:\n",
    "The sigmoid function maps the input to a value between 0 and 1, while the tanh function maps the input to a value between -1 and 1. The tanh function produces a symmetric output range, which allows it to model both positive and negative values.\n",
    "\n",
    "2. Non-linearity:\n",
    "Both the sigmoid and tanh functions introduce non-linearity to neural networks. However, the tanh function exhibits stronger non-linearity than the sigmoid function. The sigmoid function has a smooth S-shaped curve, while the tanh function has a steeper gradient and a steeper transition from negative to positive values.\n",
    "\n",
    "3. Zero-Centered Output:\n",
    "One advantage of the tanh function over the sigmoid function is that it produces zero-centered outputs. For inputs close to zero, the tanh function returns an output close to zero. This property can make the optimization process easier for the subsequent layers of the network, as the inputs are centered around zero.\n",
    "\n",
    "4. Gradient Saturation:\n",
    "Both the sigmoid and tanh functions can suffer from gradient saturation. When the input values to these functions are very large or very small, the gradients can become extremely small, hindering the learning process. However, compared to the sigmoid function, the tanh function exhibits a steeper gradient and saturates less in the intermediate range, which can help with improved gradient flow and learning.\n",
    "\n",
    "5. Computational Cost:\n",
    "The computational cost of the tanh function is higher than that of the sigmoid function. The tanh function involves the computation of exponentials, which can be more computationally expensive compared to the sigmoid function's exponential computation.\n",
    "\n",
    "In practice, the tanh function is often used as an alternative to the sigmoid function, especially in situations where zero-centered outputs and stronger non-linearity are desired. The zero-centered property of tanh can be beneficial for certain tasks, such as symmetric learning or when dealing with data with negative and positive values. However, it is important to note that the tanh function can still suffer from the vanishing gradient problem for extreme inputs, similar to the sigmoid function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8812f4-3eba-41a4-a509-7a38491eb35f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
